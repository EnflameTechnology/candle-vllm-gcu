# ğŸ•¯ï¸ Candle-vLLM-GCU

> **é¢å‘ç‡§åŸGCUå¹³å°çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸èŠå¤©æœåŠ¡æ¡†æ¶**ï¼Œå…¶åŸºäº `Candle-GCU` å’Œ å¼€æºé¡¹ç›®[`Candle-vLLM`](https://github.com/EricLBuehler/candle-vllm) å®ç°ï¼Œå…¼å®¹OpenAI APIæ¥å£ã€‚

---

<p align="center">
  <a href="./README.md">English</a> |
  <a href="./README-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ”§ æ„å»º Candle-vLLM-GCU

```bash
# å®‰è£… Rustï¼ˆéœ€ç‰ˆæœ¬ 1.88.0 æˆ–æ›´é«˜ï¼‰
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# å®‰è£…ç³»ç»Ÿä¾èµ–é¡¹
sudo apt install libssl-dev pkg-config -y

# å®‰è£…ç‡§åŸé©±åŠ¨ä¸è¿è¡Œæ—¶ç¯å¢ƒ
sudo ./TopsPlatform_1.4.xxxx.run
dpkg -i eccl_3.4.xxx_amd64.deb

# Install bindgen
cargo install bindgen-cli

# æ›´æ–°å­é¡¹ç›®
git submodule update --init --recursive
cd candle-vllm

# æ„å»ºé€‚ç”¨äºå•èŠ‚ç‚¹ï¼ˆå•å¡æˆ–å•æœºå¤šå¡ï¼‰
cargo build --release --features gcu,eccl

# æ„å»ºé€‚ç”¨äºå•èŠ‚ç‚¹ï¼ˆ+CUDA Graphç‰¹æ€§ï¼‰
cargo build --release --features gcu,eccl,graph

# æ„å»ºé€‚ç”¨äºå¤šèŠ‚ç‚¹ï¼ˆMPIï¼Œå¤šæœºå¤šå¡ï¼‰æ”¯æŒç‰ˆæœ¬
sudo apt update
sudo apt install libopenmpi-dev openmpi-bin clang libclang-dev -y
cargo build --release --features gcu,eccl,mpi
```

---

## âœ… æ”¯æŒçš„ç‰¹æ€§

- âœ… **å¤šå¡ä¸å¤šæœºå¹¶è¡Œæ¨ç†**ï¼ˆMulti-GPUs, Multi-Nodesï¼‰
- âœ… **é‡åŒ–æ”¯æŒ**ï¼ˆGPTQã€AWQï¼‰
- âœ… **è¿ç»­æ‰¹å¤„ç†**
- âœ… **åˆ†é¡µæ³¨æ„åŠ›æœºåˆ¶**
- âœ… **åˆ†å—é¢„å¡«å……**
- âœ… **KV ç¼“å­˜æ”¯æŒ**
  - âœ… BF16
  - âœ… FP16
  - âŒ æš‚ä¸æ”¯æŒ INT8
- âœ… **å…¼å®¹ OpenAI æ¥å£çš„æœåŠ¡**
- âŒ **å¤šæ¨¡æ€æ¨¡å‹**
- ğŸ› ï¸ **CUDA Graph** _(å¼€å‘ä¸­)_


## âš™ï¸ æ„å»ºåŠå¯åŠ¨å‚æ•°è¯´æ˜

- [`ENV_PARAM`] cargo run [`BUILD_PARAM`] -- [`PROGRAM_PARAM`] [`MODEL_ID/MODEL_WEIGHT_PATH`] [`MODEL_TYPE`] [`MODEL_PARAM`]  
  <details open>
    <summary>æ˜¾ç¤ºè¯¦æƒ…</summary>

    **ç¤ºä¾‹:**

    ```shell
    [RUST_LOG=warn] cargo run [--release --features gcu,eccl] -- [--log --dtype bf16 --p 2000 --d 0,1 --mem 8192] [--weight-path /home/weights/QwQ-32B]
    ```

    `ENV_PARAM`: RUST_LOG=warn

    `BUILD_PARAM`: --release --features gcu,eccl

    `PROGRAM_PARAM`ï¼š--log --dtype bf16 --p 2000 --d 0,1 --mem 8192

    `MODEL_WEIGHT_PATH`: --w /home/weights/QwQ-32B

    å…¶ä¸­ï¼Œ `--p`: æœåŠ¡ç«¯å£; `--d`: è®¾å¤‡åºåˆ—å·; `--w`: æƒé‡è·¯å¾„ (safetensorsè·¯å¾„); `--f`: æƒé‡æ–‡ä»¶ (GGUFæ¨¡å‹ä½¿ç”¨); `--m`: Huggingface model-id; `--mem`å‚æ•°æ§åˆ¶KV Cacheç¼“å­˜ï¼Œé•¿æ–‡æœ¬æˆ–æ‰¹é‡æ¨ç†é‡è¯·å¢å¤§ç¼“å­˜; `--prefill-chunk-size`æŒ‡å®šåˆ†å—prefillæ—¶çš„å—å¤§å°ï¼ˆé»˜è®¤8Kï¼Œ`0`ä¸ºç¦ç”¨ï¼‰ã€‚
  </details>

---

## ğŸ¥ èŠå¤©æ¼”ç¤ºè§†é¢‘

**ğŸ”· DeepSeek-R1 685Bï¼ˆAWQï¼Œçº¦ 8 tokens/sï¼Œ8 å¼  Enflame S60-48Gï¼Œçº¦ 120GB æƒé‡å¸è½½åˆ° CPUå†…å­˜ï¼‰** <img src="resources/DeepSeek-R1-685B-S60-Candle-vLLM-GCU.gif" width="85%">

**ğŸ”· LLaMa3.1 8Bï¼ˆAWQï¼Œçº¦ 40 tokens/sï¼Œ1 å¼  Enflame S60-48Gï¼‰** <img src="resources/LLaMa3.1-8B-S60-Quant.gif" width="85%">

---

## ğŸ“Š æ¨¡å‹æ”¯æŒä¸æ€§èƒ½

å½“å‰æ”¯æŒåœ¨ **Enflame S60 (48GB)** ä¸Šè¿è¡Œçš„æ¨¡å‹å¦‚ä¸‹ï¼š

**1k tokens æ¨ç†é•¿åº¦è¾“å‡ºç»Ÿè®¡ï¼š**

| Model ID | Model Type | Supported | Speed (BF16, bs=1)| Thoughput (BF16, bs=16) | Thoughput (W4A16)
|--|--|--|--|--|--|
| #1 | **LLAMA** |âœ…|30 tks/s (7B), 27 tks/s (LLaMa3.1 8B)| 375 tks/s (LLaMa3.1 8B) | 41 tks/s (**bs=1**), 1185 tks/s (**bs=48**)|
| #2 | **Mistral** |âœ…|29 tks/s (7B)|330 tks/s (7B)|TBD|
| #3 | **Phi (v1, v1.5, v2)** |âœ…|TBD|TBD|TBD|
| #4 | **Phi-3** |âœ…|38 tks/s (3.8B)|320 tks/s (BF16+F32, 7B)|TBD|
| #5 | **Yi** |âœ…|28 tks/s (6B)|305 tks/s (6B)|TBD|
| #6 | **StableLM** |âœ…|48 tks/s (3B)|425 tks/s (BF16, 3B)|TBD|
| #7 | BigCode/StarCode |TBD|TBD|TBD|
| #8 | ChatGLM |TBD|TBD|TBD|
| #9 | **QWen2** |âœ…|22 tks/s (14B, **tp=2**)|322 tks/s (14B, **tp=2, bs=32**)|TBD|
| #9 | **Qwen3** |âœ…|23 tks/s (8B, **bs=1**)|607 tks/s (14B, **bs=48**)|TBD|
| #10 | **Google Gemma** |âœ…|51 tks/s (2B)| 577 tks/s (2B) |TBD|
| #11 | GLM4 |âœ…|TBD|TBD|
| #12 | Moondream-2 (Multimodal LLM) |TBD|TBD|TBD|
| #13 | **DeepSeek-V3/R1** (awq 671/685B, offloading) |âœ…|~8tks/s (**tp=8**)|155tks/s (**tp=8, bs=48**)|TBD|
| #14 | **QwQ-32B** |âœ…|10.6 tokens (**tp=2**)|214 tokens (**tp=2, bs=32**)|TBD|
---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

<details open>
<summary><strong>è¿è¡Œæœªå‹ç¼©æ¨¡å‹</strong></summary>

```bash
target/release/candle-vllm --p 2000 \
--w /home/DeepSeek-R1-Distill-Llama-8B/
```

</details>

<details open>
<summary><strong>è¿è¡Œ GPTQ é‡åŒ–æ¨¡å‹</strong></summary>

```bash
# æ ¼å¼è½¬æ¢ ï¼ˆ8bit gptq -> Enflame formatï¼‰
python3 transform_safetensors.py --src /path/to/gptq \
--dst /path/to/gptq-enflame --bits 8 --method gptq --group 128 --nk True

#è¿è¡Œæ ¼å¼è½¬æ¢åçš„æ¨¡å‹
target/release/candle-vllm --dtype bf16 --p 2000 --w /path/to/gptq-enflame
```

</details>

<details open>
<summary><strong>è¿è¡Œ AWQ é‡åŒ–æ¨¡å‹</strong></summary>

```bash
# æ ¼å¼è½¬æ¢ ï¼ˆ4bit awq -> Enflame formatï¼‰
python3 transform_safetensors.py --src /path/to/awq \
--dst /path/to/awq-enflame --bits 4 --method awq --group 64 --nk True

#è¿è¡Œæ ¼å¼è½¬æ¢åçš„æ¨¡å‹
target/release/candle-vllm --dtype f16 --p 2000 --w /path/to/awq-enflame
```

</details>

## ğŸ–¥ï¸ å¤šå¡ä¸å¤šèŠ‚ç‚¹æ¨ç†æ”¯æŒ

<details open>
<summary><strong>å¤šè¿›ç¨‹å¤šå¡æ¨ç†</strong></summary>

```bash
# æŒ‡å®šå¡0å’Œå¡1
target/release/candle-vllm --p 2000 --d 0,1 --w /path/to/model
```

</details>

<details>
<summary><strong>å¤šèŠ‚ç‚¹å¤šå¡æ¨ç†ï¼ˆMPIï¼‰é…ç½®</strong></summary>

```bash
# å®‰è£… MPI
sudo apt install libopenmpi-dev openmpi-bin clang libclang-dev -y

# æ„å»ºæ”¯æŒ MPI çš„ç‰ˆæœ¬
cargo build --release --features gcu,eccl,mpi

# ä½¿ç”¨ mpirun å¯åŠ¨ï¼ˆç¡®ä¿åŒæœºä¸­æƒé‡ä¸candle-vllm binaryåœ¨ç›¸åŒç›®å½•ï¼‰
sudo mpirun -np 16 -x RUST_LOG=info -hostfile ./hostfile \
--allow-run-as-root -bind-to none -map-by slot \
--mca btl_tcp_if_include %NET_INTERFACE% \
target/release/candle-vllm --dtype bf16 --p 2000 \
--d 0,1,2,3,4,5,6,7 --w /data/deepseek-enflame deep-seek
```

</details>

---

## ğŸ’¬ èŠå¤©å‰ç«¯æ”¯æŒ

### é€‰é¡¹ 1ï¼šä½¿ç”¨ `chat.py` å¿«é€Ÿæµ‹è¯•

```bash
pip install openai rich click
python3 examples/chat.py
python3 examples/chat.py --live # Markdown å®æ—¶æ¸²æŸ“æ”¯æŒ
```

### é€‰é¡¹ 2ï¼šä½¿ç”¨ Chat UI (å¸¦èŠå¤©è®°å½•åŠŸèƒ½)

```bash
# å®‰è£…Rust aichat
cargo install aichat

aichat --serve
# é€‰æ‹© `openai-compatible`æ¨¡å¼, provide name å¡«å†™`candle-vllm`
# å°† candle-vllm API åœ°å€ï¼ˆå¦‚ http://0.0.0.0:2000/v1/ï¼‰å¡«å…¥ï¼Œ (API Keyä¸ºç©º, `LLMs to include`é€‰æ‹©default)
# ç‚¹å‡» aichatç”Ÿæˆçš„ "LLM Playground" URLåœ°å€å³å¯
```

https://github.com/user-attachments/assets/6fbad80b-e4d8-453f-b50d-50f61fa8c4f3

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡Œæ‰¹å¤„ç†åŸºå‡†æµ‹è¯•ï¼š

```bash
python3 examples/benchmark.py --batch 16 --max_tokens 1024
```

è¯¦è§è„šæœ¬ [`benchmark.py`](candle-vllm/examples/benchmark.py)ã€‚

---

## ğŸ“¦ è½¬æ¢ä¸º Enflame é‡åŒ–æ ¼å¼

1. ä½¿ç”¨ `transform_safetensors.py` å·¥å…·è½¬æ¢æ¨¡å‹ã€‚
2. ç¤ºä¾‹ï¼š

```bash
# 8bit gptq é‡åŒ–æ ¼å¼è½¬æ¢
python3 transform_safetensors.py --src /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit --dst /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit-Enflame --bits 8 --method gptq --group 128 --nk True

# 4bit awq é‡åŒ–æ ¼å¼è½¬æ¢
python3 transform_safetensors.py --src /data/DeepSeek-R1-AWQ --dst /data/DeepSeek-R1-AWQ-Enflame/ --bits 4 --method awq --group 64 --nk True

# è¿è¡Œæ ¼å¼è½¬æ¢åæ¨¡å‹
cargo run --release --features gcu -- --p 2000 \
--w /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit-Enflame
```

---

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [ ] å¢åŠ GGUFæ¨¡å‹æ”¯æŒï¼ˆå¦‚ `q4_k`é‡åŒ–æ ¼å¼ï¼‰ã€‚
* [ ] æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹ã€‚
