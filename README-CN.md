# ğŸ•¯ï¸ Candle-vLLM-GCU

> **é¢å‘ç‡§åŸGCUå¹³å°çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸èŠå¤©æœåŠ¡æ¡†æ¶**ï¼Œå…¶åŸºäº `Candle-GCU` å’Œ å¼€æºé¡¹ç›®[`Candle-vLLM`](https://github.com/EricLBuehler/candle-vllm) å®ç°ï¼Œå…¼å®¹OpenAI APIæ¥å£ã€‚

---

<p align="center">
  <a href="./README.md">English</a> |
  <a href="./README-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ”§ æ„å»º Candle-vLLM-GCU

```bash
# å®‰è£… Rustï¼ˆéœ€ç‰ˆæœ¬ 1.83.0 æˆ–æ›´é«˜ï¼‰
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# å®‰è£…ç³»ç»Ÿä¾èµ–é¡¹
sudo apt install libssl-dev pkg-config -y

# å®‰è£…ç‡§åŸé©±åŠ¨ä¸è¿è¡Œæ—¶ç¯å¢ƒ
sudo ./TopsPlatform_1.4.xxxx.run
dpkg -i eccl_3.4.xxx_amd64.deb

# å…‹éš†å¹¶æ„å»ºé¡¹ç›®
git clone git@git.enflame.cn:era/candle-vllm-gcu.git
cd candle-vllm-gcu
git submodule update --init --recursive
cd candle-vllm

# æ„å»ºé€‚ç”¨äºå•èŠ‚ç‚¹ï¼ˆå•å¡æˆ–å•æœºå¤šå¡ï¼‰
cargo build --release --features gcu,eccl

# æ„å»ºé€‚ç”¨äºå¤šèŠ‚ç‚¹ï¼ˆMPIï¼Œå¤šæœºå¤šå¡ï¼‰æ”¯æŒç‰ˆæœ¬
sudo apt update
sudo apt install libopenmpi-dev openmpi-bin clang libclang-dev -y
cargo build --release --features gcu,eccl,mpi
```

---

## âš™ï¸ å¯åŠ¨å‚æ•°è¯´æ˜

**å‘½ä»¤æ ¼å¼ï¼š**

```bash
[ENV_PARAM] cargo run [BUILD_PARAM] -- [PROGRAM_PARAM] [MODEL_WEIGHT_PATH] [MODEL_TYPE] [MODEL_PARAM]
```

**ç¤ºä¾‹ï¼š**

```bash
RUST_LOG=warn cargo run --release --features gcu,eccl -- \
--multi-process --log --dtype bf16 --port 2000 --device-ids "0,1" --kvcache-mem-gpu 8192 \
--weight-path /home/weights/QwQ32B-GPTQ-4Bit \
qwen2 --quant gptq --temperature 0.7 --penalty 1.0 --top-k 40 --top-p 0.95
```

æ”¯æŒçš„ `MODEL_TYPE` ç±»å‹ï¼š

```
["llama", "llama3", "mistral", "phi2", "phi3", "qwen2", "qwen3", "gemma", "yi", "stable-lm", "deep-seek"]
```

---

## ğŸ¥ èŠå¤©æ¼”ç¤ºè§†é¢‘

**ğŸ”· DeepSeek-R1 685Bï¼ˆAWQï¼Œçº¦ 8 tokens/sï¼Œ8 å¼  Enflame S60ï¼Œçº¦ 120GB æƒé‡å¸è½½åˆ° CPUå†…å­˜ï¼‰** <img src="resources/DeepSeek-R1-685B-S60-Candle-vLLM-GCU.gif" width="1024pt" height="695pt" >

**ğŸ”· LLaMa3.1 8Bï¼ˆAWQï¼Œçº¦ 40 tokens/sï¼Œ1 å¼  Enflame S60ï¼‰** <img src="resources/LLaMa3.1-8B-S60-Quant.gif" width="85%" height="85%" >

---

## ğŸ“Š æ¨¡å‹æ”¯æŒä¸æ€§èƒ½

å½“å‰æ”¯æŒåœ¨ **Enflame S60 (48GB)** ä¸Šè¿è¡Œçš„æ¨¡å‹å¦‚ä¸‹ï¼š

**1k tokens æ¨ç†é•¿åº¦è¾“å‡ºç»Ÿè®¡ï¼š**

| Model ID | Model Type | Supported | Speed (BF16, bs=1)| Thoughput (BF16, bs=16) | Thoughput (W4A16)
|--|--|--|--|--|--|
| #1 | **LLAMA** |âœ…|30 tks/s (7B), 27 tks/s (LLaMa3.1 8B)| 375 tks/s (LLaMa3.1 8B) | 41 tks/s (**bs=1**), 1185 tks/s (**bs=48**)|
| #2 | **Mistral** |âœ…|29 tks/s (7B)|330 tks/s (7B)|TBD|
| #3 | **Phi (v1, v1.5, v2)** |âœ…|TBD|TBD|TBD|
| #4 | **Phi-3** |âœ…|38 tks/s (3.8B)|320 tks/s (BF16+F32, 7B)|TBD|
| #5 | **Yi** |âœ…|28 tks/s (6B)|305 tks/s (6B)|TBD|
| #6 | **StableLM** |âœ…|48 tks/s (3B)|425 tks/s (BF16, 3B)|TBD|
| #7 | BigCode/StarCode |TBD|TBD|TBD|
| #8 | ChatGLM |TBD|TBD|TBD|
| #9 | **QWen2** |âœ…|22 tks/s (14B, **tp=2**)|322 tks/s (14B, **tp=2, bs=32**)|TBD|
| #9 | **Qwen3** |âœ…|23 tks/s (8B, **bs=1**)|607 tks/s (14B, **bs=48**)|TBD|
| #10 | **Google Gemma** |âœ…|51 tks/s (2B)| 577 tks/s (2B) |TBD|
| #11 | GLM4 |âœ…|TBD|TBD|
| #12 | Moondream-2 (Multimodal LLM) |TBD|TBD|TBD|
| #13 | **DeepSeek-V3/R1** (awq 671/685B, offloading) |âœ…|~8tks/s (**tp=8**)|155tks/s (**tp=8, bs=48**)|TBD|
| #14 | **QwQ-32B** |âœ…|10.6 tokens (**tp=2**)|214 tokens (**tp=2, bs=32**)|TBD|
---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

<details>
<summary><strong>è¿è¡Œæœªå‹ç¼©æ¨¡å‹</strong></summary>

```bash
target/release/candle-vllm --port 2000 \
--weight-path /home/DeepSeek-R1-Distill-Llama-8B/ \
llama3 --temperature 0. --penalty 1.0
```

</details>

<details>
<summary><strong>è¿è¡Œ GPTQ é‡åŒ–æ¨¡å‹</strong></summary>

```bash
python3 transform_safetensors.py --src /path/to/gptq \
--dst /path/to/gptq-enflame --bits 8 --method gptq --group 128 --nk True

target/release/candle-vllm --dtype bf16 --port 2000 \
--weight-path /path/to/gptq-enflame qwen2 --quant gptq \
--temperature 0. --penalty 1.0
```

</details>

<details>
<summary><strong>è¿è¡Œ AWQ é‡åŒ–æ¨¡å‹</strong></summary>

```bash
python3 transform_safetensors.py --src /path/to/awq \
--dst /path/to/awq-enflame --bits 4 --method awq --group 64 --nk True

target/release/candle-vllm --multi-process --dtype f16 --port 2000 \
--device-ids "0" --weight-path /path/to/awq-enflame llama3 \
--quant awq --temperature 0. --penalty 1.0
```

</details>

---

## ğŸ§  åŸä½é‡åŒ–ï¼ˆå®éªŒåŠŸèƒ½ï¼‰

å°†åŸå§‹æƒé‡ç›´æ¥è½¬æ¢ä¸º Enflame æ ¼å¼å¹¶åŠ è½½è¿è¡Œï¼š

```bash
cargo run --release --features gcu -- --port 2000 \
--weight-path /home/Meta-Llama-3.1-8B-Instruct/ \
llama3 --quant q8_0
```

> âš ï¸ *æç¤ºï¼š`q4_k` æ˜¯æ›´ç†æƒ³çš„é‡åŒ–æ ¼å¼ï¼Œç›®å‰æ‰¹å¤„ç†æ€§èƒ½ä»åœ¨ä¼˜åŒ–ä¸­ã€‚*

---

## ğŸ–¥ï¸ å¤šå¡ä¸å¤šèŠ‚ç‚¹æ¨ç†æ”¯æŒ

<details>
<summary><strong>å¤šè¿›ç¨‹å¤šå¡æ¨ç†</strong></summary>

```bash
target/release/candle-vllm --multi-process --port 2000 \
--device-ids "0,1" --weight-path /path/to/model llama3 \
--temperature 0. --penalty 1.0
```

</details>

<details>
<summary><strong>å¤šèŠ‚ç‚¹ï¼ˆMPIï¼‰é…ç½®</strong></summary>

```bash
# å®‰è£… MPI
sudo apt install libopenmpi-dev openmpi-bin clang libclang-dev -y

# æ„å»ºæ”¯æŒ MPI çš„ç‰ˆæœ¬
cargo build --release --features gcu,eccl,mpi

# ä½¿ç”¨ mpirun å¯åŠ¨
sudo mpirun -np 16 -x RUST_LOG=info -hostfile ./hostfile \
--allow-run-as-root -bind-to none -map-by slot \
--mca btl_tcp_if_include %NET_INTERFACE% \
target/release/candle-vllm --multi-process --dtype bf16 --port 2000 \
--device-ids "0,1,2,3,4,5,6,7" --weight-path /data/deepseek-enflame \
deep-seek --quant awq --temperature 0. --penalty 1.0
```

</details>

---

## ğŸ’¬ èŠå¤©å‰ç«¯æ”¯æŒ

### é€‰é¡¹ 1ï¼šä½¿ç”¨ `chat.py` å¿«é€Ÿæµ‹è¯•

```bash
pip install openai rich click
python3 examples/chat.py
python3 examples/chat.py --live # Markdown å®æ—¶æ¸²æŸ“æ”¯æŒ
```

### é€‰é¡¹ 2ï¼šä½¿ç”¨ Chat UI

```bash
git clone https://github.com/guoqingbao/candle-vllm-demo.git
cd candle-vllm-demo
apt install npm
npm install -g n && n stable
npm install -g pnpm
pnpm install
pnpm run dev
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡Œæ‰¹å¤„ç†åŸºå‡†æµ‹è¯•ï¼š

```bash
python3 examples/benchmark.py --batch 16 --max_tokens 1024
```

è¯¦è§è„šæœ¬ [`benchmark.py`](candle-vllm/examples/benchmark.py)ã€‚

---

## ğŸ“¦ è½¬æ¢ä¸º Enflame é‡åŒ–æ ¼å¼

1. ä½¿ç”¨ `transform_safetensors.py` å·¥å…·è½¬æ¢æ¨¡å‹ã€‚
2. ç¤ºä¾‹ï¼š

```bash
# 8bit gptq é‡åŒ–æ ¼å¼è½¬æ¢
python3 transform_safetensors.py --src /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit --dst /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit-Enflame --bits 8 --method gptq --group 128 --nk True

# 4bit awq é‡åŒ–æ ¼å¼è½¬æ¢
python3 transform_safetensors.py --src /data/DeepSeek-R1-AWQ --dst /data/DeepSeek-R1-AWQ-Enflame/ --bits 4 --method awq --group 64 --nk True

# è¿è¡Œæ ¼å¼è½¬æ¢åæ¨¡å‹
cargo run --release --features gcu -- --port 2000 \
--weight-path /data/Meta-Llama-3.1-8B-Instruct-GPTQ-8bit-Enflame llama3 --quant gptq
```

---

## ğŸ› ï¸ å¼€å‘è®¡åˆ’ï¼ˆTODOï¼‰

* [x] ä¼˜åŒ–ç”Ÿæˆé€Ÿåº¦ã€‚
* [ ] å¢åŠ æ›´å¤šé‡åŒ–æ ¼å¼æ”¯æŒï¼ˆå¦‚ `q4_k`ã€`w4a16`ï¼‰ã€‚
* [x] æ”¯æŒå¤šç”¨æˆ·åŒæ—¶å¯¹è¯ã€‚
* [ ] æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹ã€‚
